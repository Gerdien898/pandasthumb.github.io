---
title: Breakthrough for Intelligent Design? (Part 2)
date: 2022-11-16 12:00:00 -08:00
author: Lars Johan Erkell
---

<figure><img src="/uploads/2022/Biologg_Banner_900.jpg" alt="Biolog(g) banner"/>
<figcaption>The text is a pun in Swedish: "Logg" means something like a ship´s log or a logbook. A blog is "blogg". 
So "Biolog(g)" is a biologist´s log or blog.</figcaption>
</figure>

<p>&nbsp;</p>
This is part 2 of a series of 7 posts by Lars Johan Erkell, with comments on each by Ola Hössjer and a reply by Erkell.  An eighth post by Erkell is near this one, put here because he refers to it. Part 1 will be found [here](https://github.com/PandasThumb/pandasthumb.github.io/edit/source/_posts/2022/2022-11-09-erkell1.md)._

# Breakthrough for Intelligent design? (Part 2) #
November 13, 2020

by Lars Johan Erkell

__How do you calculate design?__

What stands out about Thorvaldsen and Hössjer's work, and what makes me write these blog posts, is that they raise the question of how one could demonstrate fine-tuning and design using scientific methods. In any case, how one could imagine doing so; it doesn't get more concrete than that.

Thorvaldsen and Hössjer carefully discuss the statistical methods they use, or rather believe that one can use. The main line is – as always with ID – that you try to support ID by primarily criticizing the theory of evolution. But you cannot prove theory A by showing that theory B is bad. The authors are also aware of this. They write in section 6.3:

> However, one may argue that the most suitable approach of science is to compare the best explanations found so far within two competing worldviews. This naturally leads to model selection. … It is possible for some problems to suggest a design model M1 that competes with the currently most promising naturalistic model M2, in terms of which model explains the data the best.
The authors therefore want to compare an evolutionary model with a design model by using statistical methods to determine which of them is more probable. And it is clear that the intention is to make the comparison based on concrete data. But from which models, and from which data?

In their critique of evolutionary theory, a number of well-known themes are addressed, primarily "irreducible complexity" on several levels; in individual proteins, in protein complexes and in entire networks. Also the "Cambrian explosion" and the so-called the waiting time problem is addressed. What these have in common is that the authors believe that the structures of life are so complex and thus so improbable that they could never arise naturally. However, this is not a view shared by established science – here the arguments are seen as contrived.

The question now is, in practical terms, these problems could be calculated. A question that has been dealt with extensively - and which is probably the easiest to calculate - concerns the probability that a functioning protein will be able to arise through naturalistic mechanisms. Thorvaldsen and Hössjer mention in section 4.2 two strategies for doing this:

The first is based on the fact that protein consists of one or more chains of amino acids that must be in a certain sequence. Since there are 20 different amino acids to choose from, there are countless combinations. In a small protein consisting of 150 amino acids, the different amino acids can be combined in 10<sup>50</sup> to 10<sup>74</sup> different ways. These are astronomically large numbers; the probability that the "right" combination would occur by chance is vanishingly small. The big problem with this strategy, however, is that no sane biologist believes that proteins form in this random way.

The evolutionary biological view is instead that the first enzymes consisted of short peptide chains with a small number of amino acids – it is known that such chains can have enzymatic activity.<sup>1</sup> That such peptide chains could be formed by chance is not unreasonable. They may not have been as efficient as today's enzymes, but if they had sufficient enzyme activity, they would have been selected and evolved into larger and more efficient molecules. So no one believes that proteins are formed in the way that Thorvaldsen and Hössjer are suggesting. It is a little strange that they mention this bizarre idea at all. It is completely irrelevant.

The other strategy is to set up a hypothetical evolutionary tree for how a certain protein might have evolved, and then calculate the probability of that process. But there is a lot of knowledge that is missing in order to be able to make credible calculations of this kind, not least because we have to reckon with an evolution that extends over billions of years. We would need detailed knowledge of which organisms are involved, mutation frequencies, population sizes, selection pressure and much else that is impossible to know in retrospect. This is not a realistic path. Nor, as far as I know, has anyone attempted to set up any detailed scenario of this kind.

The conclusion is that it is not practically possible to statistically calculate the probability that a protein could evolve via natural mechanisms. The difficulties are so enormous that it will probably never be possible. And yet this was the simplest of the cases discussed.

<div align="center">***</div>
Thorvaldsen and Hössjer's strategy for demonstrating fine-tuning is based on a statistical comparison between a naturalistic model M2 and a design model M1. We then need to calculate the probability for both models in order to compare them. We have already seen that it is practically impossible to calculate the probability that a complex biological structure can arise according to a naturalistic model. But how to calculate the likelihood of a design model? It is something the authors wisely do not go into. Here we are in big trouble.
In order to be able to calculate the probability of how a complex biological structure can arise, we must have a credible and detailed model of the process. This also applies if there is a conscious design behind it. We must therefore have knowledge of the designer in order to assess the probability that he/she/it wanted the design exactly the way we see it. This requires detailed knowledge – we need to understand exactly how the designer thinks, what the intentions, plans and means are. In order to make statistical calculations, we must have actual data, numbers.
Design proponents, however, are careful not to say anything about who the designer might be, or what qualities the designer might have. This makes it fundamentally impossible to calculate the statistical probability of a design process. So it's not just practically impossible, as with the naturalistic model, it's fundamentally impossible.
The statistical model Thorvaldsen and Hössjer have put forward to demonstrate design is misconceived and unworkable – it is based on the unreasonable assumption that it is possible to make a statistical estimate of the probability that an unknown designer would have acted in a certain way. It is a mystery that they chose to publish it.
Notes:
1. Short peptides can spontaneously form stable complexes that have enzymatic activity. See: Jason Greenwald et. al.: Peptide Amyloids in the Origin of Life. J Mol Biol (2018) vol. 430, pp. 3735–3750 [here](https://pubmed.ncbi.nlm.nih.gov/29890117/)

<p>&nbsp;</p>

__Comment by Ola Hössjer__
June 29, 2021 3:00 a.m

__Part 2. About counting on design__

In order to be able to estimate the probability that a biological system should have arisen by chance (so step a) in the fine-tuning argument) a naturalistic model M2 is required that describes how development or evolution took place. The fact that the model is naturalistic means that it contains only natural explanatory mechanisms. When it comes to biological evolution, i.e. the development of complicated life forms from simple single-celled organisms, the five naturalistic mechanisms are usually involved i) mutations, ii) genetic drift, i.e. random variation in how reproductive the individuals in a parental generation are, iii) natural selection, iv) recombinations of genetic material from a pair of parents, and v) the mixing of gene pools caused by geographic migration.
Erkell claims that it is practically impossible to give a reasonable estimate of the probability that, for example, a protein could have evolved in accordance with a naturalistic model M2. I agree that this is extremely difficult, since such a model must necessarily be complicated, with more or less unknown parameters. Nevertheless, I believe that there is a feasible way to demonstrate the difficulty of naturalistic processes in producing complex structures. If you only focus on certain properties of the protein that according to M2 must have evolved, you get a conservative upper estimate of the probability that a protein was obtained through evolutionary processes. To also take into account the fact that several of the model's parameters (such as selection pressure, mutation rate, population sizes, generation times, etc.) can be difficult to estimate from the data with good accuracy, a so-called sensitivity analysis can be used to calculate how the upper estimate of the probability for the development of the protein changes when the constituent parameters change. If this upper estimate of the probability is small for all biologically reasonable values of the included parameters, an argument has been given for the difficulty of the proposed naturalistic model M2 to explain the origin of the given protein. I therefore argue, contrary to Erkell, that the first step a) of our method for detecting fine-tuning is highly feasible in practice.
The second part b) of our fine-tuning argument consists in finding a measure f(x) of how specified a biological system x is. As mentioned in the introduction, f(x) fastens on some characteristic that the observer recognizes, and which in many cases brings to mind that an intelligent designer is behind the origin of the system. In TH2020 we give several examples of how the system x and its specification f(x) can be chosen. An example is that x corresponds to a protein or a molecular machine, and that the specification indicates how many parts must be in place for x to function. Another example is that f(x) indicates the biological fitness of an organism x, i.e. its reproductive capacity. In connection with the latter example, we argue in TH2020 that evolution will eventually break down the genetic material of a species because most mutations are harmful or neutral. The consequence is then that the fitness of individuals of the species decreases over time. This in turn means that the probability in a) becomes very low for an evolutionary model M2, based on the aforementioned mechanisms i)-v), to be able to explain that the organism in question arose during millions of years of development, because the genetic material during such long time has had time to be depleted.
But if a naturalistic model M2 has difficulty explaining the emergence of a biological system that contains some form of specification, the question is whether there is any other better explanatory model. At the end of TH2020, we give examples of design- or creationist-inspired models M1 that we believe have greater possibilities than M2 to explain the emergence of the fine-tuned system. To make such reasoning more rigorous, it is necessary to show that the probability of the emergence of the given system is much higher for the design model M1 than for the naturalistic model M2. In the example above, where f(x) corresponds to the biological fitness of an organism x, we refer to various articles where a design model is described, which in addition to the aforementioned (micro)evolutionary mechanisms i)-v) adds a sixth mechanism vi) in the form of genetic variation in a first created pair, from which all individuals in the species descend. According to such a design model M1, the emergence of species can be explained by a combination of created variation (mechanism vi) and a short-term, small-scale microevolution (mechanisms i-v). The problem of genetic depletion then becomes much less, because the created variation makes it possible to assume that the species arose relatively recently, so that the genetic erosion, caused by the harmful mutations, acted for a much shorter time<sup>2</sup>.
Erkell is strongly critical of us introducing a design model to explain the emergence of a biological system. He writes:
> In order to be able to calculate the probability of how a complex biological structure can arise, we must therefore have a credible and detailed model of the process. This also applies if there is a conscious design behind it. We must therefore have knowledge of the designer in order to assess the probability that the person concerned wanted his design exactly the way we see it to be. This requires detailed knowledge – we need to understand exactly how the designer thinks, what intentions, plans and means they have. In order to make statistical calculations, we must have concrete data, numbers.
>
> However, the design movement is very careful not to say anything about who the designer would be, or what qualities the designer would have. This makes it fundamentally impossible to calculate the statistical probability of a design process. It is thus not only practically impossible, as with the naturalistic model, it is fundamentally impossible.
I completely agree with Erkell that it is difficult to determine exactly what plans or intentions a designer has. But this is also not necessary to know. It is enough to let design-inspired thinking generate concrete falsifiable hypotheses. A design-inspired model can therefore be perfectly combined with the hypothetical-deductive method, which forms the basis of empirical science, according to the following three steps:
1) Propose a falsifiable hypothesis, which can contain both natural mechanisms and design-inspired parts.
2) Evaluate and see if the hypothesis in 1) is consistent with the data.
3) If the agreement in 2) is not good, go back to 1) and propose a better hypothesis.
Erkell makes very far-reaching, not to say unreasonable demands on the design model, because he is not satisfied with falsifiable hypotheses. Instead, he also wants to include characteristics of the designer that can often be difficult to test scientifically. Although it is probably so interesting to have a conversation about what intentions the designers have, this has to do with the interpretation and the reason for the emergence of hypotheses, which is not required for the hypothetico-deductive method. One can thus use more or less intuitive design arguments to formulate testable hypotheses, without having to test the reasoning that led to the formulation of the hypotheses.
Furthermore, Erkell's reasoning backfires like a boomerang on himself. Shouldn't he make the same rigorous demands on naturalistic models M2, where the interpretation of them is included? How, for example, should chance in an evolutionary model be explained? Is it epistemic chance (as a way of describing lack of knowledge), ontological chance (genuine non-determinism) or should quantum mechanics be used to explain the emergence of chance? Which interpretation of quantum mechanics should be used in that case? If these questions cannot be given an unequivocal answer, Erkell risks sawing off the secular branch of science he himself has set himself up for.
2) This is usually called Haldane's dilemma. A historical overview of Fisher's Fundamental Theorem of Natural selection (which is closely related to Haldane's Dilemma) is given in Basener W., Cordova S., Hössjer O. and Sanford J. (2021). Dynamical Systems and Fitness Maximization in Evolutionary Biology. In: Sriraman B. (eds) Handbook of the Mathematics of the Arts and Sciences. Running, Cham.

<p>&nbsp;</p>

__Reply by Lars Johan Erkell__
September 8, 2021 6:12 am

In this section, we get into the core of the work, your statistical model to detect fine-tuning/design. It assumes that you can calculate the probability that a biological structure can evolve via natural mechanisms, an extremely difficult task. You write that you can make meaningful estimates of the probability of a protein being able to evolve by doing a sensitivity analysis of an upper estimate of this probability. If the variation of this estimate is small for all biologically plausible values of the involved parameters, one would have an argument for the difficulty of the proposed naturalistic model.
But how do you get "all biologically reasonable values"? It is not enough to count on a single gene – genes (and proteins) interact in complex networks, and selection pressure acts on the entire network. The interaction between genes is called epistasis, and it must be included in your model. Ecology is also crucial because the selection pressure is entirely dependent on the organism's environment. The problem is that there are astronomically many possibilities for epistatic (and ecological) interactions, and you have no clues as to which might be the relevant ones. You cannot count on a certain protein "all else being equal"; because the genome is constantly changing, "all else is not equal". All of this means that you are nowhere near a realistic model of an evolution that has taken place in countless steps over millions or billions of years.
Apart from all this, there is also a fundamental flaw in the approach, namely that it assumes the probability of a process with a definite goal in the form of a definite structure (at least that's how ID advocates tend to calculate). But evolutionary processes have no goal. As far as proteins and protein complexes are concerned, there are a huge number of structures that have the same or similar function, and it is one of these that will be selected. It is thus the probability that the function - not the exact structure - will evolve that is important.
In any case, you mean that "our method of detecting fine-tuning is highly feasible". If so, why haven't you done so? Why don't you at least have a discussion of which factors would be included in such a calculation, how they should be identified, how they should be estimated, how they should be weighed against each other, etc.? Then—and only then—would this article be interesting. It would be even more interesting if you could succeed with the second part of your method, namely calculating the probability of design. If you are to test the probability that a certain structure is designed, you must know the characteristics of the designer - design is an act of will, and it is the probability of that act of will that you must calculate. And that is of course impossible without in-depth knowledge of the designer. You don't write anything about how this would work, and that doesn't surprise me; I suspect you have no idea. At least I don't have it. But this is necessary if you want to be able to compare M1 and M2. That comparison is the very core of your method. But you comment on it like this:
"Erkell makes very far-reaching, not to say unreasonable, demands on the design model, because he is not satisfied with falsifiable hypotheses. Instead, he also wants to include characteristics of the designer that can often be difficult to test scientifically."
There's no getting away from this: the probability of a design hypothesis includes the probability that a designer really wants to accomplish a certain thing. Without intention no design. If we imagine an omnipotent designer (which you probably do) it is only an act of will; no practical complications exist. But I get curious; you write "characteristics of the designer that can often be difficult to test scientifically", which must mean that there are actually characteristics of the designer that can be scientifically tested. What are they? How can you test them? Here we get into really central questions about scientific hypotheses. That discussion is so important and extensive that it hardly fits in the comments section. I have therefore written [a separate post](https://github.com/PandasThumb/pandasthumb.github.io/edit/source/_posts/2022/2022-11-09-erkell8.md) on the matter, "Science, hypotheses and Intelligent Design".
In your comment you also touch on completely different things: you talk about "genetic erosion" and refer in a footnote to "Haldane's dilemma". Here you are confusing two things. Haldane's dilemma is not about some kind of erosion 
but about how fast evolution can go, more specifically whether there was enough time for the gene substitutions that must have taken place from the time of the common ancestor of humans and chimpanzees until today. The genetic erosion you talk about is John Sanford's genetic entropy, a completely different matter. It would consist of a slow and inevitable genetic decay that would lead to our demise. However, neither Sanford nor anyone else has been able to show that this decay actually occur. Sanford builds on an earlier, now dismissed, idea that a genetic "catastrophe" could occur if enough harmful mutations accumulate in a genome. However, it has never been shown that such catastrophes actually take place, but rather that they do not (Springman, R. et al: Evolution at a High Imposed Mutation Rate: Adaptation Obscures the Load in Phage T7. Genetics (2010)184: 221 –232); Summers, J., Litwin, S.: Examining The Theory of Error Catastrophe. J. Virol., (2006) 80, 20–260). So forget about genetic entropy.
In conclusion, you suggest that I should place the same strict demands on my own research as I do on yours. But that's exactly what I do - I make the same demands on your research that I myself have to live up to. Regarding your question about chance, I (and my colleagues) assume an ontological chance, since chemical reactions are based on the interaction between elementary particles and these are subject to the quantum laws in their so-called. Copenhagen interpretation. This is the basis of the mole
